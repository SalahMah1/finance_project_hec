{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder as ohe\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "past = pd.read_csv('PastLoans.csv')\n",
    "new = pd.read_csv('NewApplications_1_Round1.csv')\n",
    "past.set_index(['id'], inplace=True)\n",
    "new.set_index(['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(past.shape)\n",
    "# print(past.head())\n",
    "# print(new.shape)\n",
    "# print(new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "past1 = past.drop(['digital2', 'digital3'], axis=1)\n",
    "past1.rename(columns={'digital1':'digital'}, inplace=True)\n",
    "y1 = list(past1['default'])\n",
    "X1 = past1.drop(['default'], axis=1)\n",
    "\n",
    "past2 = past.drop(['digital1', 'digital3'], axis=1)\n",
    "past2.rename(columns={'digital2':'digital'}, inplace=True)\n",
    "y2 = list(past2['default'])\n",
    "X2 = past2.drop(['default'], axis=1)\n",
    "\n",
    "past3 = past.drop(['digital1', 'digital2'], axis=1)\n",
    "past3.rename(columns={'digital3':'digital'}, inplace=True)\n",
    "y3 = list(past3['default'])\n",
    "X3 = past3.drop(['default'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['sex', 'employment']\n",
    "pass_features = ['married']\n",
    "numerical_features = ['income']\n",
    "numerical_pass_features = ['digital']\n",
    "\n",
    "cat_encoder = ohe()\n",
    "numerical_encoder = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1[numerical_features] = numerical_encoder.fit_transform(X1[numerical_features])\n",
    "X2[numerical_features] = numerical_encoder.fit_transform(X2[numerical_features])\n",
    "X3[numerical_features] = numerical_encoder.fit_transform(X3[numerical_features])\n",
    "\n",
    "dums = pd.get_dummies(X1[cat_features], drop_first=True)\n",
    "X1.drop(cat_features, axis=1, inplace=True)\n",
    "X2.drop(cat_features, axis=1, inplace=True)\n",
    "X3.drop(cat_features, axis=1, inplace=True)\n",
    "\n",
    "X1[dums.columns] = dums\n",
    "X2[dums.columns] = dums\n",
    "X3[dums.columns] = dums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat1 = ohe().fit(X1[cat_features])\n",
    "# cat2 = ohe().fit(X2[cat_features])\n",
    "# cat3 = ohe().fit(X3[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=42).fit(X_train1, y_train1)\n",
    "clf2 = LogisticRegression(random_state=42).fit(X_train2, y_train2)\n",
    "clf3 = LogisticRegression(random_state=42).fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestClassifier\n",
    "AdaBoostClassifier\n",
    "GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distributions = dict(C=uniform(loc=0, scale=4), penalty=['l2', 'l1'])\n",
    "rand_clf1 = RandomizedSearchCV(clf1, distributions, random_state=42)\n",
    "search = rand_clf1.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01961447, 0.01359619, 0.01834862, 0.0176331 , 0.00886162])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf1, X_train1, y_train1, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4603174603174603"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test1, clf1.predict(X_test1), average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.score(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.predict(X_test1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot = TPOTClassifier()\n",
    "tpot.fit(X_train, y_train)\n",
    "tpot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpot \n",
    "\n",
    "\n",
    "tpot = TPOTClassifier()\n",
    "tpot.fit(X_train1, y_train1)\n",
    "tpot.score(X_test1, y_test1)\n",
    "\n",
    "# # let's try tpot\n",
    "# # define evaluation procedure\n",
    "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# # define search\n",
    "# model = TPOTRegressor(generations=5, population_size=50, scoring='neg_mean_absolute_error', cv=cv, verbosity=2, random_state=1, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 operators have been imported by TPOT.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 '(slice(None, None, None), 0)' is an invalid key.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Optimization Progress:  10%|â–‰         | 105/1100 [13:54<1:34:59,  5.73s/pipeline]"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "import timeit\n",
    "\n",
    "# create train and test sets\n",
    "\n",
    "tpot = TPOTClassifier(verbosity=3, \n",
    "                      scoring=\"f1_macro\", \n",
    "                      random_state=42,\n",
    "                      n_jobs=-1, \n",
    "                      generations=10, \n",
    "                      population_size=100)\n",
    "# run three iterations and time them\n",
    "for x in range(3):\n",
    "    start_time = timeit.default_timer()\n",
    "    tpot.fit(X_train1, y_train1)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    times.append(elapsed)\n",
    "    winning_pipes.append(tpot.fitted_pipeline_)\n",
    "    print('yo')\n",
    "    scores.append(tpot.score(X_test1, y_test1))\n",
    "    tpot.export('tpot_mnist_pipeline.py')\n",
    "times = [time/60 for time in times]\n",
    "print('Times:', times)\n",
    "print('Scores:', scores)   \n",
    "print('Winning pipelines:', winning_pipes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "096d4302486a5f76f9b50d582eb8507d0f454f0513db05604f74a7cecd9e1d14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
